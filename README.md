![GitHub Workflow Status](https://github.com/Moon-hub-dot/data-pipeline-project/actions/workflows/run_pipeline.yml/badge.svg)

# ğŸ› ï¸ Data Pipeline Project â€“ Automated Data Cleaning & Deployment

Welcome to the **Data Monitoring Pipeline Project** built by **Shivani G** â€“ a fresher passionate about data analysis, DevOps, and cloud automation. This project simulates a real-world data engineering workflow using Python, Shell scripting, GitHub Actions, and AWS.

---

## ğŸ“Œ Project Highlights

- âœ… **Automated Data Cleaning** using Python (pandas)
- âœ… **Log Generation** with time-stamped summaries
- âœ… **Shell Script** to simulate pipeline automation
- âœ… **GitHub Actions CI/CD** for auto-execution on every code push
- âœ… [Optional] AWS S3 & Lambda trigger setup for cloud automation

---

## ğŸ“‚ Folder Structure

data_pipeline_project/
â”‚
â”œâ”€â”€ raw_data/ # Raw input CSV files
â”œâ”€â”€ cleaned_data/ # Output: Cleaned CSVs
â”œâ”€â”€ logs/ # Logs and summary reports
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ data_cleaner.py # Core cleaning + logging script
â”‚ â””â”€â”€ lambda_trigger.py # Simulated AWS Lambda trigger
â”œâ”€â”€ run_pipeline.sh # Shell script to execute pipeline
â”œâ”€â”€ .github/
â”‚ â””â”€â”€ workflows/
â”‚ â””â”€â”€ run_pipeline.yaml # GitHub Actions automation
â””â”€â”€ README.md # You're reading it!
---

## ğŸ” Workflow Overview

### ğŸ”¹ Step 1: Data Cleaning with Python
- Reads raw sales data CSV using `pandas`
- Cleans missing values, trims whitespaces
- Generates summary:
  - Total Sales
  - Total Profit
  - Top 5 Cities
- Saves cleaned data + summary log file

### ğŸ”¹ Step 2: Automation with Shell Script
- Executes Python script
- Simulates upload to AWS S3
- Echoes completion status

### ğŸ”¹ Step 3: GitHub Actions CI/CD
- Automatically runs the pipeline on every `git push`
- Future-ready for CRON jobs or scheduled triggers

---

## â˜ï¸ (Optional) Cloud Component: AWS

- **AWS S3**: Simulated upload of cleaned data files
- **AWS Lambda**: Simulated auto-trigger when new file arrives in bucket

(Note: For real AWS usage, you'd need free-tier or paid access.)

---

## ğŸ“Š Sample Summary Log
âœ… Summary:
Total Orders: 999
Total Sales: â‚¹3,52,450.00
Total Profit: â‚¹54,800.40
Top 5 Cities: {'New York': 120, 'San Francisco': 98, 'Los Angeles': 94, ...}
---

## ğŸš€ How to Run Locally

1. Clone repo  
   `git clone https://github.com/Moon-hub-dot/data-pipeline-project.git`

2. Navigate into folder  
   `cd data_pipeline_project`

3. Install dependencies  
   `pip install pandas`

4. Run the pipeline  
   - On Windows: `bash run_pipeline.sh` (Git Bash recommended)

---

## ğŸ§  Skills Demonstrated

- **Python** (Data wrangling with pandas)
- **DevOps** (Shell scripting, CI/CD)
- **Cloud Concepts** (AWS S3, Lambda â€“ simulated)
- **Git & GitHub** (Version control, Actions)

---

## ğŸ“„ Resume Add-on

**â€œBuilt a complete data pipeline that automates data cleaning using Python, logs summaries, and simulates cloud upload to AWS S3. Integrated GitHub Actions for CI/CD and explored AWS Lambda-based automation triggers.â€**

---

## ğŸ™‹â€â™€ï¸ About Me

ğŸ‘©ğŸ»â€ğŸ’» **Shivani G**  
ğŸ“ BCA Graduate | Aspiring Data Analyst  
ğŸ“§ shivaniganesh388@gmail.com  
ğŸ”— [LinkedIn](www.linkedin.com/in/shivani-g-2bb2a9288)  
ğŸŒŸ Always learning. Always growing.

---

## ğŸ«±ğŸ¼â€ğŸ«²ğŸ¼ Contributions

Pull requests, suggestions, and forks are welcome!  
â­ Star this repo if you found it helpful.


